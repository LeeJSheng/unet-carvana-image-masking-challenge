{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.grid'] = False\n",
    "matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "\n",
    "from sklearn.model_selection import train_test_split  #function to split datalist into train and test set\n",
    "from PIL import Image\n",
    "\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfcontrib\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data\n",
    "Let us look at the Carvana Image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories of training images as well as training masks\n",
    "train_dir = \"train\"\n",
    "mask_dir = \"train_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract a list of image IDs\n",
    "train_df = pd.read_csv(\"train_masks.csv\")\n",
    "train_ids = train_df['img'].map(lambda s : s.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example of a car and its 16 images\n",
    "plt.figure(figsize=(70, 40))\n",
    "i = 0\n",
    "for img_id in train_ids[:16]:\n",
    "    img_path = os.path.join('train', (img_id + '.jpg'))\n",
    "    img = mplimg.imread(img_path)\n",
    "\n",
    "    # show the image\n",
    "    plt.subplot(6, 8, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Id: {}\".format(img_id), fontsize=20)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "plt.suptitle(\"16 images of a car taken at 22.5Â° rotation\",y=0.94,fontsize=48)\n",
    "#plt.savefig(\"sample car 16 images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Validation sets\n",
    "We'll need to split the training image and labels into training and validation sets\n",
    "\n",
    "Validation set is just to make sure our model isn't overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames = []    # list of training image file name\n",
    "y_train_filenames = []    # list of mask image file name\n",
    "\n",
    "# format the filenames by adding the appropriate formatting to the image ids\n",
    "for id in train_ids:\n",
    "    x_train_filenames.append(os.path.join(train_dir, (id + \".jpg\")))\n",
    "    y_train_filenames.append(os.path.join(mask_dir, (id + \"_mask.gif\")))\n",
    "    \n",
    "# using sklearn's training/validation data splitter\n",
    "seed = 42\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=seed)\n",
    "\n",
    "num_train_examples = len(x_train)\n",
    "num_val_examples = len(x_val)\n",
    "\n",
    "print('Number of training samples: ' + str(num_train_examples))\n",
    "print('Number of validation samples: ' + str(num_val_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some Images and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_num = 4    # number of images to display\n",
    "\n",
    "r_choices = np.random.choice(len(x_train), display_num)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i in range(0, display_num * 2, 2):\n",
    "    img_num = r_choices[i // 2]\n",
    "    x_pathname = x_train[img_num]\n",
    "    y_pathname = y_train[img_num]\n",
    "    \n",
    "    plt.subplot(display_num, 4, i + 1)\n",
    "    plt.imshow(mplimg.imread(x_pathname))\n",
    "    plt.title(\"Original Image\")\n",
    "  \n",
    "    plt.subplot(display_num, 4, i + 2)\n",
    "    plt.imshow(mplimg.imread(y_pathname))\n",
    "    plt.title(\"Mask\")\n",
    "    \n",
    "plt.suptitle(\"Examples of Images and their Masks\", y=0.94, fontsize=24)\n",
    "#plt.savefig(\"sample car & mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an input pipeline to generate datasets with tf.data\n",
    "\n",
    "To feed data into our model, we make use of `tf.data`'s dataset pipelining capabilities with functional APIs\n",
    "\n",
    "Steps in the pipeline:\n",
    "\n",
    "1. Read (load) image files from path\n",
    "2. Decode `jpeg` and `gif` into tensors\n",
    "3. Apply Image augmentation to help model generalize better\n",
    "4. Shuffle the data, batch the data, fetch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & decode images from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_images(x_path, y_path):\n",
    "    '''load function that loads and decodes image files from their path'''\n",
    "    # decode the original jpeg image\n",
    "    x_file = tf.read_file(x_path)\n",
    "    x_img = tf.image.decode_jpeg(x_file, channels=3)\n",
    "    \n",
    "    # decode the mask .gif images\n",
    "    y_file = tf.read_file(y_path)\n",
    "    \n",
    "    # decoding a .gif file is more complicated\n",
    "    # decode_gif returns a tensor [frame_number, height, width, channel]\n",
    "    y_img = tf.image.decode_gif(y_file)[0]    # since this is not an animated gif, we take the first and only frame\n",
    "    y_img = y_img[:,:,0]                      # We take the first channel only\n",
    "    \n",
    "    # add an additional dimension to the tensor so that it has the same shape as x_img\n",
    "    y_img = tf.expand_dims(y_img, axis=-1)\n",
    "    \n",
    "    return x_img, y_img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "There are many commonly used types of data augmentation. Given the data, we will only perform the following:\n",
    "\n",
    "1. **Resize** - this is needed to ensure dimensions matches model and also due to hardware restrictions (outlined below)\n",
    "2. **Pixel Scaling** - Image values are between 0-255, rescale to \\[0,1\\] easier to training.\n",
    "3. **Image translations (shifts)** - Random horizontal & vertical translations\n",
    "4. **hue delta** - Randomly adjust RGB values to provide hue variations\n",
    "\n",
    "### Image translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_img(x_img, y_img, w_shift_range, h_shift_range):\n",
    "    \"\"\"This fn performs horizontal and vertical shift\"\"\"\n",
    "    \n",
    "    # if shift range is specified\n",
    "    if w_shift_range or h_shift_range:\n",
    "        \n",
    "        # sample a random shift amount from range\n",
    "        if w_shift_range:\n",
    "            w_shift = tf.random_uniform([], img_shape[1] * -w_shift_range, img_shape[1] * w_shift_range)\n",
    "        \n",
    "        if h_shift_range:\n",
    "            h_shift = tf.random_uniform([], img_shape[0] * -h_shift_range, img_shape[0] * h_shift_range)\n",
    "        \n",
    "        # apply shift\n",
    "        x_img = tfcontrib.image.translate(x_img, [w_shift, h_shift])\n",
    "        y_img = tfcontrib.image.translate(y_img, [w_shift, h_shift])\n",
    "        \n",
    "    return x_img, y_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine augmentations into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augment(x_img,\n",
    "             y_img,\n",
    "             resize=None,       # resize to [h,w]\n",
    "             scale=1/255,       # Scale pixel values of image\n",
    "             hue_delta=0,       # Adjust hue by a random factor\n",
    "             w_shift_range=0,   # random horizontal translation\n",
    "             h_shift_range=0):  # random vertical translation\n",
    "    \n",
    "    # Resize\n",
    "    if resize is not None:\n",
    "        x_img = tf.image.resize_images(x_img, resize)\n",
    "        y_img = tf.image.resize_images(y_img, resize)\n",
    "    \n",
    "    # hue shift\n",
    "    if hue_delta:\n",
    "        x_img = tf.image.random_hue(x_img, hue_delta)\n",
    "    \n",
    "    # image shift\n",
    "    x_img, y_img = shift_img(x_img, y_img, w_shift_range, h_shift_range)\n",
    "    \n",
    "    # pixel scaling\n",
    "    x_img = tf.to_float(x_img) * scale\n",
    "    y_img = tf.to_float(y_img) * scale\n",
    "    \n",
    "    return x_img, y_img\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Pipeline\n",
    "\n",
    "Now we stitch the functions into a pipeline that generates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(x_filenames,\n",
    "               y_filenames,\n",
    "               preproc_fn=functools.partial(_augment),\n",
    "               threads=5,\n",
    "               batch_size=10,\n",
    "               shuffle=True):\n",
    "    \n",
    "    num_x = len(x_filenames)\n",
    "    \n",
    "    # create the filename queue\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_filenames, y_filenames))\n",
    "    \n",
    "    # add file reading and decoding to the queue\n",
    "    dataset = dataset.map(_load_images, num_parallel_calls=threads)\n",
    "    \n",
    "    # add any preprocessing to the queue\n",
    "    dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(num_x)\n",
    "        \n",
    "    # repeat data for all epoch\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "Now that we've defined out pipeline, now it's time to build our model with building blocks.\n",
    "\n",
    "A U-Net consists of stacking **Convolutional Layers** into **Encoder Blocks** and **Decoder Blocks**\n",
    "Which we will define as functions that generate tensor graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use functional API of Keras to define and build building blocks\n",
    "\n",
    "# A single convolution block with 2 convolution layer.\n",
    "def conv_block(input_tensor, num_filters=32, kernel_size=(3,3)):\n",
    "    # Convolution 1\n",
    "    output = layers.Conv2D(num_filters, kernel_size, padding='same')(input_tensor)\n",
    "    output = layers.BatchNormalization()(output)\n",
    "    output = layers.Activation('relu')(output)\n",
    "    \n",
    "    # Convolution 2\n",
    "    output = layers.Conv2D(num_filters, kernel_size, padding='same')(output)\n",
    "    output = layers.BatchNormalization()(output)\n",
    "    output = layers.Activation('relu')(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# encoder block\n",
    "def encoder_block(input_tensor, num_filters, kernel_size):\n",
    "    # each encoder block performs 1 convolusion block\n",
    "    output = conv_block(input_tensor, num_filters, kernel_size)\n",
    "    pool = layers.MaxPooling2D((2,2), strides=(2,2))(output)\n",
    "    \n",
    "    # return the output to retrain information to be fed into the decoding layers\n",
    "    return pool, output\n",
    "\n",
    "# decoder block\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters=32, kernel_size=(3,3)):\n",
    "    # transpose convolution for upsampling\n",
    "    upsampled = layers.Conv2DTranspose(num_filters,(2,2),strides=(2,2),padding='same')(input_tensor)\n",
    "    \n",
    "    # Concatenate the upsampled feature map with the pre-downsampled feature map\n",
    "    concat = layers.concatenate([concat_tensor, upsampled], axis=-1)\n",
    "    output = layers.BatchNormalization()(concat)\n",
    "    output = layers.Activation('relu')(output)\n",
    "    \n",
    "    # decoder Convolusion block\n",
    "    output = conv_block(output, num_filters, kernel_size)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build UNet Segmentation CNN Architecture\n",
    "def u_net(img_shape):\n",
    "    # img_dim\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "\n",
    "    # img_dim/2, img_dim\n",
    "    encode0_pool, encode0 = encoder_block(inputs, 32, (3,3))\n",
    "    # img_dim/4, img_dim/2\n",
    "    encode1_pool, encode1 = encoder_block(encode0_pool, 64, (3,3))\n",
    "    # img_dim/8, img_dim/4\n",
    "    encode2_pool, encode2 = encoder_block(encode1_pool, 128, (3,3))\n",
    "    # img_dim/16, img_dim/8\n",
    "    encode3_pool, encode3 = encoder_block(encode2_pool, 256, (3,3))\n",
    "    # img_dim/32, img_dim/16\n",
    "    encode4_pool, encode4 = encoder_block(encode3_pool, 512, (3,3))\n",
    "\n",
    "    # img_dim/32, centre block\n",
    "    centre = conv_block(encode4_pool, 1024, (3,3))\n",
    "\n",
    "    # img_dim/16, input: img_dim/32, concated: img_dim/16\n",
    "    decode4 = decoder_block(centre, encode4, 512, (3,3))\n",
    "    # img_dim/8, input: img_dim/16, concated: img_dim/8\n",
    "    decode3 = decoder_block(decode4, encode3, 256, (3,3))\n",
    "    # img_dim/4, input: img_dim/8, concated: img_dim/4\n",
    "    decode2 = decoder_block(decode3, encode2, 128, (3,3))\n",
    "    # img_dim/2, input: img_dim/4, concated: img_dim/2\n",
    "    decode1 = decoder_block(decode2, encode1, 64, (3,3))\n",
    "    # img_dim, input: img_dim/2, concated: img_dim\n",
    "    decode0 = decoder_block(decode1, encode0, 32, (3,3))\n",
    "\n",
    "    # output segmentation pixel generation\n",
    "    outputs = layers.Conv2D(1, (1,1), activation='sigmoid')(decode0)\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss functions\n",
    "For Image Segmentation problems the dice score is usually used. This score measures the segmentation overlaps and works better for **imbalanced** problems.\n",
    "\n",
    "Dice loss is the loss function version of dice score.\n",
    "\n",
    "We'll use a custom loss function combining **binary cross entropy** with **dice loss**. This is based on empirical tries of other contestants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    # Flatten to reduce dimensionality\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "    \n",
    "    # calculate the dice coefficient\n",
    "    smooth = 1.0\n",
    "    intersect = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    dice = (2.0 * intersect + smooth) / (tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat) + smooth)\n",
    "    \n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dice metric into a loss function for training\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coef(y_true, y_pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and compile\n",
    "Now that we've defined out pipeline, model and loss functions, it's time to instantiate them as objects\n",
    "\n",
    "## Setup and Generate train and validation datasets\n",
    "This UNet architecture requires the size of image be evenly divisible by 32, as downsampling by factor of 2 happens 5 times.\n",
    "\n",
    "Given my machine limitation, dimension of 128x192 is used. Higher resolution is better, resize to a larger dimension divisible by 32 if your machine supports it.\n",
    "\n",
    "Alternatively tweak the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "img_shape = (128, 192, 3)\n",
    "batch_size = 3\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset configurations\n",
    "train_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1/255.,\n",
    "    'hue_delta': 0.1,\n",
    "    'w_shift_range': 0.1,\n",
    "    'h_shift_range': 0.1\n",
    "}\n",
    "train_preproc_fn = functools.partial(_augment, **train_cfg)\n",
    "\n",
    "# validation dataset configurations\n",
    "val_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1/255.\n",
    "}\n",
    "val_preproc_fn = functools.partial(_augment, **val_cfg)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = get_dataset(x_train,\n",
    "                           y_train,\n",
    "                           preproc_fn=train_preproc_fn,\n",
    "                           batch_size=batch_size)\n",
    "val_dataset = get_dataset(x_val,\n",
    "                         y_val,\n",
    "                         preproc_fn=val_preproc_fn,\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our pipeline and visualize augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ds = get_dataset(x_train,\n",
    "                     y_train,\n",
    "                     preproc_fn=train_preproc_fn,\n",
    "                     batch_size=2,\n",
    "                     shuffle=False)\n",
    "\n",
    "data_aug_iter = temp_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    batch_of_imgs, label = sess.run(next_element)\n",
    "    \n",
    "    plt.figure(figsize=(14,10))\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(batch_of_imgs[0])\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(label[0,:,:,0])\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(batch_of_imgs[1])\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(label[1,:,:,0])\n",
    "    \n",
    "    plt.suptitle(\"Sample Training Data\",y=0.94,fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Compile Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model object given network definition earlier\n",
    "model_inputs, model_outputs = u_net(img_shape)\n",
    "model = models.Model(inputs=[model_inputs], outputs=[model_outputs])\n",
    "\n",
    "# compile with training params\n",
    "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss, dice_coef])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Once all objects has been defined, we can start training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logs & model saving objects\n",
    "# make sure this path has been CREATED, keras doesn create it for you\n",
    "save_model_path = './tmp/train1/weights-{epoch:02d}-{val_dice_loss:.2f}.hdf5'\n",
    "\n",
    "# callback object to save keras model, save best model based on validation dice loss\n",
    "keras_cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path,\n",
    "                                             monitor='val_dice_loss',\n",
    "                                             save_best_only=True,\n",
    "                                             verbose=1)\n",
    "\n",
    "# tensorboard checkpoint to allow graph visualizations and parameter histograms\n",
    "tensorboard_cp = tf.keras.callbacks.TensorBoard(log_dir='./logs/train6',\n",
    "                                               histogram_freq=1,\n",
    "                                               batch_size=batch_size\n",
    "                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_dataset,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    callbacks=[keras_cp,tensorboard_cp],\n",
    "                    validation_data=val_dataset,\n",
    "                    steps_per_epoch=int(np.ceil(num_train_examples / batch_size)),\n",
    "                    validation_steps=int(np.ceil(num_val_examples / batch_size))\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history of custom binary-cross-entrophy + dice loss\n",
    "losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "\n",
    "# history of dice loss\n",
    "dice_losses = history.history['dice_loss']\n",
    "val_dice_losses = history.history['val_dice_loss']\n",
    "\n",
    "# history of dice score\n",
    "dice_scores = history.history['dice_coef']\n",
    "val_dice_scores = history.history['val_dice_coef']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "# plot bce + dice losses\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs_range, losses, label='Train')\n",
    "plt.plot(epochs_range, val_losses, label='Val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation BCE + Dice Loss')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,0.15])\n",
    "\n",
    "# plot dice losses\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs_range, dice_losses, label='Train')\n",
    "plt.plot(epochs_range, val_dice_losses, label='Val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Dice Loss')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,0.08])\n",
    "\n",
    "# plot dice scores\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs_range, dice_scores, label='Train')\n",
    "plt.plot(epochs_range, val_dice_scores, label='Val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Dice Scores')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.94,1])\n",
    "\n",
    "plt.suptitle(\"Training Metrices\",y=0.98,fontsize=18)\n",
    "plt.savefig('50 epoch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_model = models.load_model(\"./tmp/weights-03-1.00.hdf5\",\n",
    "#                              custom_objects={\n",
    "#                                  'bce_dice_loss': bce_dice_loss,\n",
    "#                                  'dice_loss': dice_loss\n",
    "#                              })\n",
    "model.load_weights(\"./tmp/train1/weights-45-0.01.hdf5\")    # key in the correct model name and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize some of the outputs \n",
    "data_aug_iter = val_dataset.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "\n",
    "# visualize the images, actual masks & predicted masks\n",
    "plt.figure(figsize=(15, 15))\n",
    "num_to_show = 4\n",
    "for i in range(num_to_show):\n",
    "    # get a batch of image\n",
    "    batch_of_imgs, masks = tf.keras.backend.get_session().run(next_element)\n",
    "    \n",
    "    # take the first image of the batch prediction\n",
    "    predicted_mask = model.predict(batch_of_imgs)[0]\n",
    "    predicted_mask_np = np.array(predicted_mask)\n",
    "    predicted_mask_np = np.round(predicted_mask_np, 0)\n",
    "    \n",
    "    # show the image\n",
    "    img = batch_of_imgs[0]\n",
    "    plt.subplot(num_to_show, 3, 3 * i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Image\")\n",
    "    #plt.axis(\"off\")\n",
    "    \n",
    "    # show the actual mask\n",
    "    plt.subplot(num_to_show, 3, 3 * i + 2)\n",
    "    plt.imshow(masks[0, :, :, 0], cmap='gray')\n",
    "    plt.title(\"Actual Mask\")\n",
    "    #plt.axis(\"off\")\n",
    "    \n",
    "    # show the predicted mask\n",
    "    plt.subplot(num_to_show, 3, 3 * i + 3)\n",
    "    plt.imshow(predicted_mask_np[:, :, 0], cmap='gray')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    #plt.axis(\"off\")\n",
    "    \n",
    "plt.suptitle(\"Samples of Validation Image, Actual Mask, and Predicted Mask\",y=0.94,fontsize=18)\n",
    "plt.savefig('50 epoch sample valid data result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our model with test images\n",
    "Let's test our model on test image and change the input size to see how well it performs with scale variations.\n",
    "\n",
    "Keep in mind the input dimensions need to be evenly divisible by 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shapes = [(128, 192, 3), (256, 384, 3), (512, 768,3), (1024, 1536,3), (1280,1920,3)]\n",
    "test_img_shape = test_shapes[0]\n",
    "\n",
    "# redefine the model with new input image size\n",
    "t_inputs, t_outputs = u_net(test_img_shape)\n",
    "test_model = models.Model(inputs=[t_inputs], outputs=[t_outputs])\n",
    "\n",
    "# load model weights\n",
    "test_model.load_weights(\"./tmp/train1/weights-45-0.01.hdf5\")    # load correct model name and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of test data colours: light-bronze, white, black, red, light-blue\n",
    "test_img_ids = ['00b6aee52419', '0bb87dac6ad9', '0a0e3fb8f782', '0a63454298b8','0b9d03ec6720']\n",
    "sample_num = ['_01','_02','_03','_04','_05','_06','_07','_08','_09','_10','_11','_12','_13','_14','_15','_16']\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "for test_img_id in test_img_ids:\n",
    "    figure_fname = '50 epoch test samples ' + test_img_id + '@{}x{}'.format(test_img_shape[1],test_img_shape[0])\n",
    "    for i in range(16):\n",
    "        test_img_path = os.path.join('test', (test_img_id + sample_num[i] + '.jpg'))\n",
    "        test_img_original = mplimg.imread(test_img_path)\n",
    "\n",
    "        # right pad so width from 1918 to 1920\n",
    "        test_img = np.pad(test_img_original, [(0,0),(0,2),(0,0)], 'constant')\n",
    "        # resize\n",
    "        test_img = Image.fromarray(test_img).resize((test_img_shape[1],test_img_shape[0]),Image.BILINEAR)\n",
    "        # scale pixels & expand_dimension to fit model\n",
    "        test_img = np.expand_dims(np.array(test_img)*1/255, axis=0)\n",
    "\n",
    "        # prediction\n",
    "        predicted_mask = test_model.predict(test_img)[0]\n",
    "        predicted_mask_np = np.array(predicted_mask)\n",
    "        predicted_mask_np = np.round(predicted_mask_np, 0)\n",
    "\n",
    "        # show the image\n",
    "        if i < 8: plt.subplot(6, 8, i+1)\n",
    "        else: plt.subplot(6,8,i+17)\n",
    "        plt.imshow(test_img[0])\n",
    "        plt.title(\"Image\", fontsize=20)\n",
    "\n",
    "        # show the predicted mask\n",
    "        if i < 8: plt.subplot(6, 8, i+9)\n",
    "        else: plt.subplot(6,8,i+25)\n",
    "        plt.imshow(predicted_mask[:, :, 0])\n",
    "        plt.title(\"Predicted Mask Raw\", fontsize=20)\n",
    "\n",
    "        # show the image\n",
    "        if i < 8: plt.subplot(6, 8, i+17)\n",
    "        else: plt.subplot(6,8,i+33)\n",
    "        plt.imshow(predicted_mask_np[:, :, 0])\n",
    "        plt.title(\"Predicted Mask Rounded\", fontsize=20)\n",
    "\n",
    "    plt.suptitle(\"Prediction on test sample {} @{}x{}\".format(test_img_id, test_img_shape[1],test_img_shape[0]),y=0.92,fontsize=48)\n",
    "    plt.savefig(figure_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that if our model is trained on images scaled down too much, it performs poorly on larger scaled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission to Kaggle\n",
    "If our model is trained on smaller images, we'll just have to resize the prediction mask into full-scale image for Kaggle submission.\n",
    "\n",
    "It's ugly, but it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run length encoder to encode our predictions for Kaggle submission\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (128, 192, 3) # input image shape\n",
    "\n",
    "# define model object given network definition earlier\n",
    "model_inputs, model_outputs = u_net(img_shape)\n",
    "model = models.Model(inputs=[model_inputs], outputs=[model_outputs])\n",
    "model.load_weights(\"./tmp/train1/weights-45-0.01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fnames = os.listdir('test')\n",
    "test_num = len(test_fnames)\n",
    "\n",
    "with open('submission2.csv', mode='w', newline='') as submission_file:\n",
    "    submission_writer = csv.writer(submission_file, delimiter=',')\n",
    "    # write header row\n",
    "    submission_writer.writerow(['img','rle_mask'])\n",
    "    \n",
    "    count = 1\n",
    "    for test_fname in test_fnames:\n",
    "        test_img_path = os.path.join('test', test_fname)\n",
    "        test_img = mplimg.imread(test_img_path)\n",
    "        \n",
    "        # input image prprocessing\n",
    "        # right pad so width from 1918 to 1920\n",
    "        test_img = np.pad(test_img, [(0,0),(0,2),(0,0)], 'constant')\n",
    "        # resize\n",
    "        test_img = Image.fromarray(test_img).resize(img_shape,Image.BILINEAR)\n",
    "        # scale pixels & expand_dimension to fit model\n",
    "        test_img = np.expand_dims(np.array(test_img)*1/255, axis=0)\n",
    "\n",
    "        # prediction\n",
    "        predicted_mask = model.predict(test_img)[0]\n",
    "        \n",
    "        # predicted mask post-processing\n",
    "        # scale pixels up to use PIL Image object\n",
    "        predicted_mask = predicted_mask * 255\n",
    "        # convert datatype\n",
    "        predicted_mask = predicted_mask.astype('uint8')\n",
    "        # resize with PIL\n",
    "        predicted_mask = Image.fromarray(predicted_mask[:,:,0], mode=\"L\").resize((1920,1280),Image.BILINEAR)\n",
    "        # scale pixels back down down to [0,1]\n",
    "        predicted_mask = np.array(predicted_mask) * 1/255.0\n",
    "        # round to 0 or 1\n",
    "        predicted_mask = np.round(predicted_mask, 0)[:,:1918]\n",
    "        \n",
    "        # encode to rle and write to file\n",
    "        encoded = rle_encode(predicted_mask)\n",
    "        submission_writer.writerow([test_fname,encoded])\n",
    "        \n",
    "        # since we'll be processing 100,064 images, this is just to check that the code is still running\n",
    "        if count % 100 == 0:\n",
    "            print('Predicted & encoded ' + str(count) + ' files of ' + str(test_num))\n",
    "            \n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowGPU]",
   "language": "python",
   "name": "conda-env-tensorflowGPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
